{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c04560-547d-4e0e-b8fb-827b43d34f04",
   "metadata": {},
   "source": [
    "Query 1\n",
    "---\n",
    "Να ταξινομημηθούν, σε φθίνουσα σειρά, οι ηλικιακές ομάδες των θυμάτων σε περιστατικά που περιλαμβάνουν οποιαδήποτε μορφή “βαριάς σωματικής βλάβης”. Θεωρείστε τις εξής ηλικιακές ομάδες:\n",
    "- **Παιδιά:** < 18\n",
    "- **Νεαροί ενήλικοι:** 18 – 24\n",
    "- **Ενήλικοι:** 25 – 64\n",
    "- **Ηλικιωμένοι:** >64\n",
    "---\n",
    "Question 1\n",
    "---\n",
    "Να υλοποιηθεί το Query 1 χρησιμοποιώντας τα DataFrame και RDD APIs. Να εκτελέσετε και\n",
    "τις δύο υλοποιήσεις με 4 Spark executors. Υπάρχει διαφορά στην επίδοση μεταξύ των δύο APIs;\n",
    "Αιτιολογήσετε την απάντησή σας. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9171add9-de2f-48ab-ac43-ae76b9dfcae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3839</td><td>application_1732639283265_3779</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3779/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3779_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3840</td><td>application_1732639283265_3780</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3780/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3780_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3843</td><td>application_1732639283265_3783</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3783/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3783_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3850</td><td>application_1732639283265_3790</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3790/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3790_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3855</td><td>application_1732639283265_3795</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3795/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3795_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3872</td><td>application_1732639283265_3812</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3812/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3812_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3873</td><td>application_1732639283265_3813</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3813/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3813_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3874</td><td>application_1732639283265_3814</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3814/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3814_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5661d4f1-5547-4972-a046-8f34bee411d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3875</td><td>application_1732639283265_3815</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3815/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3815_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|   Age_Group| count|\n",
      "+------------+------+\n",
      "|      Adults|121052|\n",
      "|Young adults| 33588|\n",
      "|    Children| 15918|\n",
      "|     Elderly|  5985|\n",
      "+------------+------+\n",
      "\n",
      "Time taken for Dataframe API: 4.43 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "\n",
    "# Load crime data from S3\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_data = crime_df.union(crime_df2)\n",
    "\n",
    "crime_data = crime_data.filter((col(\"LON\") != 0) & (col(\"LAT\") != 0))\n",
    "\n",
    "start_time = time.time()\n",
    "# Filter records with \"aggravated assault\" in the description\n",
    "agg_assault = crime_data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\")) \\\n",
    "    .filter(col(\"Vict Age\") >= 0)\n",
    "\n",
    "# Categorize ages into groups\n",
    "age_groups = agg_assault.select(\n",
    "    when(col(\"Vict Age\") < 18, \"Children\") \\\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young adults\") \\\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\") \\\n",
    "    .otherwise(\"Elderly\").alias(\"Age_Group\")\n",
    ")\n",
    "\n",
    "# Count and sort age groups\n",
    "age_group_counts = age_groups.groupBy(\"Age_Group\").count() \\\n",
    "                              .orderBy(col(\"count\").desc())\n",
    "\n",
    "# Show results\n",
    "age_group_counts.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29487d59-f695-4777-ab36-7a510b6cd869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults: 121052\n",
      "Young adults: 33588\n",
      "Children: 15918\n",
      "Elderly: 5985\n",
      "Time taken for RDD API: 15.38 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Convert to RDD and perform operations\n",
    "rdd = crime_data.rdd.map(lambda row: (row[\"Crm Cd Desc\"], row[\"Vict Age\"])) \\\n",
    "                    .filter(lambda x: \"AGGRAVATED ASSAULT\" in x[0]) \\\n",
    "                    .filter(lambda x: x[1] >= 0) \\\n",
    "                    .map(lambda x: (\"Children\" if x[1] < 18 else\n",
    "                                    \"Young adults\" if 18 <= x[1] <= 24 else\n",
    "                                    \"Adults\" if 25 <= x[1] <= 64 else\n",
    "                                    \"Elderly\")) \\\n",
    "                    .countByValue()\n",
    "\n",
    "# Sort and print\n",
    "for group, count in sorted(rdd.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for RDD API: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a523a6-9d40-4d63-99c9-51959d818a9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Query 2\n",
    "---\n",
    "\n",
    "Να βρεθούν, για κάθε έτος, τα 3 Αστυνομικά Τμήματα με το υψηλότερο ποσοστό κλεισμένων (περατωμένων) υποθέσεων. Να τυπωθούν το έτος, τα ονόματα (τοποθεσίες) των τμημάτων, τα ποσοστά τους καθώς και οι αριθμοί του ranking τους στην ετήσια κατάταξη. Τα αποτελέσματα να δοθούν σε σειρά αύξουσα ως προς το έτος και το ranking (δείτε παράδειγμα στον Πίνακα 2).\n",
    "\n",
    "---\n",
    "Question 2\n",
    "---\n",
    "- Να υλοποιηθεί το Query 2 χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και\n",
    "να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων.\n",
    "- Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet2 file format και\n",
    "αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από\n",
    "τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους\n",
    "εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cf7bdb-6a44-4e52-8dd7-ffed6e3ce7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----------+------------------+----+\n",
      "|YEAR|  AREA NAME|closed_cases|total_cases|  closed_case_rate|rank|\n",
      "+----+-----------+------------+-----------+------------------+----+\n",
      "|2010|    Rampart|        2860|       8706|    32.85090742017|   1|\n",
      "|2010|    Olympic|        2762|       8764|31.515289821999087|   2|\n",
      "|2010|     Harbor|        2818|       9598| 29.36028339237341|   3|\n",
      "|2011|    Olympic|        2798|       7987| 35.03192688118192|   1|\n",
      "|2011|    Rampart|        2744|       8443|32.500296103280824|   2|\n",
      "|2011|     Harbor|        2806|       9840|28.516260162601625|   3|\n",
      "|2012|    Olympic|        2923|       8523|34.295435879385195|   1|\n",
      "|2012|    Rampart|        2791|       8598|32.461037450569904|   2|\n",
      "|2012|     Harbor|        2781|       9416|29.534834324553948|   3|\n",
      "|2013|    Olympic|        2789|       8305| 33.58217940999398|   1|\n",
      "|2013|    Rampart|        2616|       8148|  32.1060382916053|   2|\n",
      "|2013|     Harbor|        2504|       8429| 29.70696405267529|   3|\n",
      "|2014|   Van Nuys|        3031|       9471| 32.00295639320029|   1|\n",
      "|2014|West Valley|        2504|       7946|31.512710797885727|   2|\n",
      "|2014|    Mission|        3113|       9972| 31.21740874448456|   3|\n",
      "|2015|   Van Nuys|        3383|      10485|32.265140677157845|   1|\n",
      "|2015|    Mission|        3245|      10651|30.466622852314334|   2|\n",
      "|2015|   Foothill|        2356|       7762|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|        3382|      10507|  32.1880650994575|   1|\n",
      "|2016|West Valley|        3045|       9696|31.404702970297027|   2|\n",
      "+----+-----------+------------+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for Dataframe API: 9.38 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, year, to_date\n",
    "import pyspark.sql.functions as f\n",
    "# Correct the column name \"AREA \" to \"AREA\" if necessary\n",
    "crime_data = crime_data.withColumn(\"YEAR\", year(to_date(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "\n",
    "start_time = time.time()\n",
    "dept_closed = crime_data.select(\"YEAR\", \"AREA NAME\", \"Status\") \\\n",
    "                   .groupBy(\"YEAR\", \"AREA NAME\") \\\n",
    "                   .agg(\n",
    "                        f.sum(f.when((col(\"Status\") != \"IC\") & (col(\"Status\") != \"UNK\"), 1)).alias(\"closed_cases\"),\n",
    "                        f.count(\"*\").alias(\"total_cases\")\n",
    "                    ) \\\n",
    "                    .withColumn(\n",
    "                        \"closed_case_rate\",\n",
    "                        (col(\"closed_cases\") / col(\"total_cases\")) * 100\n",
    "                    )\n",
    "\n",
    "windowSpec = Window.partitionBy(\"YEAR\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "result = dept_closed.select(\"*\", rank().over(windowSpec).alias(\"rank\")) \\\n",
    "                    .filter(col(\"rank\") <= 3) \\\n",
    "                    .orderBy(col(\"YEAR\").asc(), col(\"rank\").asc())\n",
    "\n",
    "result.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63614ae-859c-412d-bbe6-db2bba345bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----------+-----------------+----+\n",
      "|YEAR|  AREA NAME|closed_cases|total_cases| closed_case_rate|rank|\n",
      "+----+-----------+------------+-----------+-----------------+----+\n",
      "|2010|    Rampart|        2860|       8706|32.85090742017000|   1|\n",
      "|2010|    Olympic|        2762|       8764|31.51528982199909|   2|\n",
      "|2010|     Harbor|        2818|       9598|29.36028339237341|   3|\n",
      "|2011|    Olympic|        2798|       7987|35.03192688118192|   1|\n",
      "|2011|    Rampart|        2744|       8443|32.50029610328082|   2|\n",
      "|2011|     Harbor|        2806|       9840|28.51626016260163|   3|\n",
      "|2012|    Olympic|        2923|       8523|34.29543587938519|   1|\n",
      "|2012|    Rampart|        2791|       8598|32.46103745056990|   2|\n",
      "|2012|     Harbor|        2781|       9416|29.53483432455395|   3|\n",
      "|2013|    Olympic|        2789|       8305|33.58217940999398|   1|\n",
      "|2013|    Rampart|        2616|       8148|32.10603829160530|   2|\n",
      "|2013|     Harbor|        2504|       8429|29.70696405267529|   3|\n",
      "|2014|   Van Nuys|        3031|       9471|32.00295639320030|   1|\n",
      "|2014|West Valley|        2504|       7946|31.51271079788573|   2|\n",
      "|2014|    Mission|        3113|       9972|31.21740874448456|   3|\n",
      "|2015|   Van Nuys|        3383|      10485|32.26514067715784|   1|\n",
      "|2015|    Mission|        3245|      10651|30.46662285231434|   2|\n",
      "|2015|   Foothill|        2356|       7762|30.35300180365885|   3|\n",
      "|2016|   Van Nuys|        3382|      10507|32.18806509945750|   1|\n",
      "|2016|West Valley|        3045|       9696|31.40470297029703|   2|\n",
      "+----+-----------+------------+-----------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for SQL API: 3.84 seconds"
     ]
    }
   ],
   "source": [
    "# Register temp views\n",
    "crime_data.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "start_time = time.time()\n",
    "# SQL Query\n",
    "sql_result = spark.sql(\"\"\"\n",
    "WITH crime_data_year AS (\n",
    "    SELECT\n",
    "        `YEAR`,\n",
    "        `AREA NAME`,\n",
    "        `Status`\n",
    "    FROM crime\n",
    "),\n",
    "dept_closed AS (\n",
    "    SELECT\n",
    "        `YEAR`,\n",
    "        `AREA NAME`,\n",
    "        SUM(CASE WHEN `Status` NOT IN ('IC', 'UNK') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        COUNT(*) AS total_cases,\n",
    "        (SUM(CASE WHEN `Status` NOT IN ('IC', 'UNK') THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS closed_case_rate\n",
    "    FROM crime_data_year\n",
    "    GROUP BY `YEAR`, `AREA NAME`\n",
    "),\n",
    "ranked_data AS (\n",
    "    SELECT\n",
    "        `YEAR`,\n",
    "        `AREA NAME`,\n",
    "        closed_cases,\n",
    "        total_cases,\n",
    "        closed_case_rate,\n",
    "        RANK() OVER (PARTITION BY `YEAR` ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM dept_closed\n",
    ")\n",
    "SELECT\n",
    "    `YEAR`,\n",
    "    `AREA NAME`,\n",
    "    closed_cases,\n",
    "    total_cases,\n",
    "    closed_case_rate,\n",
    "    rank\n",
    "FROM ranked_data\n",
    "WHERE rank <= 3\n",
    "ORDER BY `YEAR` ASC, rank ASC;\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for SQL API: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ad0910-3bd0-4b27-9560-0d42342dd500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write results to S3 -> \n",
    "#    1. create the output directory in your S3 bucket\n",
    "#    2. change your group number below \n",
    "#    3. and uncomment\n",
    "group_number = \"14\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/crime_data/\"\n",
    "crime_data.write.mode(\"overwrite\").parquet(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a4d2e2-9e53-4571-a266-26de9c089d95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crime_data_parquet = spark.read.parquet(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95bf876-9368-4393-88f2-f698077187d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----------+------------------+----+\n",
      "|YEAR|  AREA NAME|closed_cases|total_cases|  closed_case_rate|rank|\n",
      "+----+-----------+------------+-----------+------------------+----+\n",
      "|2010|    Rampart|        2860|       8706|    32.85090742017|   1|\n",
      "|2010|    Olympic|        2762|       8764|31.515289821999087|   2|\n",
      "|2010|     Harbor|        2818|       9598| 29.36028339237341|   3|\n",
      "|2011|    Olympic|        2798|       7987| 35.03192688118192|   1|\n",
      "|2011|    Rampart|        2744|       8443|32.500296103280824|   2|\n",
      "|2011|     Harbor|        2806|       9840|28.516260162601625|   3|\n",
      "|2012|    Olympic|        2923|       8523|34.295435879385195|   1|\n",
      "|2012|    Rampart|        2791|       8598|32.461037450569904|   2|\n",
      "|2012|     Harbor|        2781|       9416|29.534834324553948|   3|\n",
      "|2013|    Olympic|        2789|       8305| 33.58217940999398|   1|\n",
      "|2013|    Rampart|        2616|       8148|  32.1060382916053|   2|\n",
      "|2013|     Harbor|        2504|       8429| 29.70696405267529|   3|\n",
      "|2014|   Van Nuys|        3031|       9471| 32.00295639320029|   1|\n",
      "|2014|West Valley|        2504|       7946|31.512710797885727|   2|\n",
      "|2014|    Mission|        3113|       9972| 31.21740874448456|   3|\n",
      "|2015|   Van Nuys|        3383|      10485|32.265140677157845|   1|\n",
      "|2015|    Mission|        3245|      10651|30.466622852314334|   2|\n",
      "|2015|   Foothill|        2356|       7762|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|        3382|      10507|  32.1880650994575|   1|\n",
      "|2016|West Valley|        3045|       9696|31.404702970297027|   2|\n",
      "+----+-----------+------------+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for Dataframe API using Parquet: 1.77 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dept_closed_parquet = crime_data_parquet.select(\"YEAR\", \"AREA NAME\", \"Status\") \\\n",
    "                   .groupBy(\"YEAR\", \"AREA NAME\") \\\n",
    "                   .agg(\n",
    "                        f.sum(f.when((col(\"Status\") != \"IC\") & (col(\"Status\") != \"UNK\"), 1)).alias(\"closed_cases\"),\n",
    "                        f.count(\"*\").alias(\"total_cases\")\n",
    "                    ) \\\n",
    "                    .withColumn(\n",
    "                        \"closed_case_rate\",\n",
    "                        (col(\"closed_cases\") / col(\"total_cases\")) * 100\n",
    "                    )\n",
    "\n",
    "windowSpec = Window.partitionBy(\"YEAR\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "result_parquet = dept_closed_parquet.select(\"*\", rank().over(windowSpec).alias(\"rank\")) \\\n",
    "                    .filter(col(\"rank\") <= 3) \\\n",
    "                    .orderBy(col(\"YEAR\").asc(), col(\"rank\").asc())\n",
    "\n",
    "result_parquet.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using Parquet: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743c633-89cc-417a-8e6c-2cf276c42dcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Query 3\n",
    "---\n",
    "\n",
    "Χρησιμοποιώντας ως αναφορά τα δεδομένα της απογραφής 2010 για τον πληθυσμό και εκείνα της απογραφής του 2015 για το εισόδημα ανα νοικοκυριό, να υπολογίσετε για κάθε περιοχή του Los Angeles τα παρακάτω:\n",
    "- Το μέσο ετήσιο εισόδημα ανά άτομο\n",
    "- Την αναλογία συνολικού αριθμού εγκλημάτων ανά άτομο.\n",
    "Τα αποτελέσματα να συγκεντρωθούν σε ένα πίνακα.\n",
    "\n",
    "---\n",
    "Question 3\n",
    "---\n",
    "\n",
    "Να υλοποιηθεί το Query 3 χρησιμοποιώντας DataFrame ή SQL API. Χρησιμοποιήστε τις μεθόδους hint & explain για να βρείτε ποιες στρατηγικές join χρησιμοποιεί ο catalyst optimizer.\n",
    "Πειραματιστείτε αναγκάζοντας το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές (μεταξύ των BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL) και σχολιάστε τα αποτελέσματα\n",
    "που παρατηρείτε. Ποιά (ή ποιές) από τις διαθέσιμες στρατηγικές join του Spark είναι καταλληλότερη(ες) και γιατί;\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e297c0e7-2d5f-4682-a5f4-a53852461e96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------------------+\n",
      "|               COMM|Crime Rate|Estimated_Median_Income|\n",
      "+-------------------+----------+-----------------------+\n",
      "|  Pacific Palisades|    0.3797|      70673.55587323125|\n",
      "|Palisades Highlands|    0.1878|      66867.43986433603|\n",
      "|   Marina Peninsula|    0.6000|      65235.69402813004|\n",
      "|            Bel Air|    0.3992|      63041.33809466166|\n",
      "|      Beverly Crest|    0.3690|      60947.49019768682|\n",
      "|          Brentwood|    0.4059|      60846.85469997952|\n",
      "|  Mandeville Canyon|    0.2611|      55572.10949582431|\n",
      "|        Playa Vista|    0.5004|      50264.47187990141|\n",
      "|            Carthay|    0.7629|     49848.737445871004|\n",
      "|             Venice|    1.0404|     47625.344329326406|\n",
      "|       Century City|    0.6330|      45617.76047098402|\n",
      "|      Playa Del Rey|    0.7426|        45522.596580114|\n",
      "|        Studio City|    0.7834|     44638.477679882526|\n",
      "|    Hollywood Hills|    0.7476|      43023.69992828971|\n",
      "|      South Carthay|    0.7232|     39831.340037649854|\n",
      "|   West Los Angeles|    0.6190|      39729.36480234261|\n",
      "|        Rancho Park|    1.0086|     39037.730430606694|\n",
      "|       Miracle Mile|    0.6094|      38849.36290573796|\n",
      "|             Encino|    0.6238|      38341.62713425434|\n",
      "|       Sherman Oaks|    0.6462|      37817.06528635011|\n",
      "+-------------------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for Dataframe API: 30.76 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, format_number, round\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "crime_data = crime_data.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the file from s3\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "LA_areas = flattened_df.select(\"ZCTA10\",\"COMM\",\"POP_2010\",\"HOUSING10\",\"geometry\") \\\n",
    "                .filter(((col(\"POP_2010\")>0) & (col(\"HOUSING10\")>0)) & (trim(col(\"COMM\"))!=\"\")) \\\n",
    "                .groupBy([\"ZCTA10\",\"COMM\"]) \\\n",
    "                .agg(\n",
    "                    ST_Union_Aggr(\"geometry\").alias(\"geometry\"),\n",
    "                    f.sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                    f.sum(\"POP_2010\").alias(\"Population\")\n",
    "                )\n",
    "\n",
    "income = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(col(\"Estimated Median Income\"), r\"[\\$,]\", \"\").cast(\"int\")) \\\n",
    "    .select(\n",
    "        col(\"Zip Code\").alias(\"ZIP\"),\n",
    "        col(\"Estimated Median Income\").alias(\"Median_Income\")\n",
    "    )\n",
    "\n",
    "joined_income = LA_areas.join(income, LA_areas[\"ZCTA10\"] == income[\"ZIP\"], \"inner\") \\\n",
    "    .drop(\"ZCTA10\", \"ZIP\") \\\n",
    "    .withColumn(\"Total_Income\", col(\"Households\") * col(\"Median_Income\")) \\\n",
    "    .drop(\"Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        f.sum(\"Population\").alias(\"Population\"),\n",
    "        f.sum(\"Total_Income\").alias(\"Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    ")\n",
    "\n",
    "population_data = joined_income.join(crime_data, ST_Within(crime_data.geom, joined_income.geometry), \"inner\") \\\n",
    "    .groupBy(\"COMM\",\"Population\",\"Total_Income\") \\\n",
    "    .agg(\n",
    "        f.count(col(\"*\")).alias(\"Total Crimes\")\n",
    "    ) \\\n",
    "    .withColumn(\"Crime Rate\",format_number((col(\"Total Crimes\")/col(\"Population\")),4)) \\\n",
    "    .withColumn(\"Estimated_Median_Income\", (col(\"Total_Income\") / col(\"Population\"))) \\\n",
    "    .drop(\"Total_Income\",\"Population\",\"Total crimes\") \\\n",
    "    .orderBy(col(\"Estimated_Median_Income\").desc())\n",
    "\n",
    "population_data.show()\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "671961d6-1b1a-4434-82d5-d76899be9faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (33)\n",
      "+- Sort (32)\n",
      "   +- Exchange (31)\n",
      "      +- HashAggregate (30)\n",
      "         +- Exchange (29)\n",
      "            +- HashAggregate (28)\n",
      "               +- Project (27)\n",
      "                  +- RangeJoin (26)\n",
      "                     :- Filter (18)\n",
      "                     :  +- ObjectHashAggregate (17)\n",
      "                     :     +- Exchange (16)\n",
      "                     :        +- ObjectHashAggregate (15)\n",
      "                     :           +- Project (14)\n",
      "                     :              +- BroadcastHashJoin Inner BuildRight (13)\n",
      "                     :                 :- ObjectHashAggregate (8)\n",
      "                     :                 :  +- Exchange (7)\n",
      "                     :                 :     +- ObjectHashAggregate (6)\n",
      "                     :                 :        +- Project (5)\n",
      "                     :                 :           +- Filter (4)\n",
      "                     :                 :              +- Generate (3)\n",
      "                     :                 :                 +- Filter (2)\n",
      "                     :                 :                    +- Scan geojson  (1)\n",
      "                     :                 +- BroadcastExchange (12)\n",
      "                     :                    +- Project (11)\n",
      "                     :                       +- Filter (10)\n",
      "                     :                          +- Scan csv  (9)\n",
      "                     +- Union (25)\n",
      "                        :- Project (21)\n",
      "                        :  +- Filter (20)\n",
      "                        :     +- Scan csv  (19)\n",
      "                        +- Project (24)\n",
      "                           +- Filter (23)\n",
      "                              +- Scan csv  (22)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#11883]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#11883]\n",
      "Condition : ((size(features#11883, true) > 0) AND isnotnull(features#11883))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#11883]\n",
      "Arguments: explode(features#11883), false, [features#11891]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#11891]\n",
      "Condition : ((((isnotnull(features#11891.properties.CITY) AND isnotnull(features#11891.properties.POP_2010)) AND isnotnull(features#11891.properties.HOUSING10)) AND ((features#11891.properties.CITY = Los Angeles) AND (((features#11891.properties.POP_2010 > 0) AND (features#11891.properties.HOUSING10 > 0)) AND NOT (trim(features#11891.properties.COMM, None) = )))) AND isnotnull(features#11891.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#11891.properties.ZCTA10 AS ZCTA10#11924, features#11891.properties.COMM AS COMM#11907, features#11891.properties.POP_2010 AS POP_2010#11916L, features#11891.properties.HOUSING10 AS HOUSING10#11913L, features#11891.geometry AS geometry#11894]\n",
      "Input [1]: [features#11891]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [ZCTA10#11924, COMM#11907, POP_2010#11916L, HOUSING10#11913L, geometry#11894]\n",
      "Keys [2]: [ZCTA10#11924, COMM#11907]\n",
      "Functions [3]: [partial_st_union_aggr(geometry#11894, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1b4b7b85, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None), partial_sum(HOUSING10#11913L), partial_sum(POP_2010#11916L)]\n",
      "Aggregate Attributes [3]: [buf#12300, sum#12302L, sum#12304L]\n",
      "Results [5]: [ZCTA10#11924, COMM#11907, buf#12301, sum#12303L, sum#12305L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [ZCTA10#11924, COMM#11907, buf#12301, sum#12303L, sum#12305L]\n",
      "Arguments: hashpartitioning(ZCTA10#11924, COMM#11907, 1000), ENSURE_REQUIREMENTS, [plan_id=6984]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [ZCTA10#11924, COMM#11907, buf#12301, sum#12303L, sum#12305L]\n",
      "Keys [2]: [ZCTA10#11924, COMM#11907]\n",
      "Functions [3]: [st_union_aggr(geometry#11894, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1b4b7b85, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None), sum(HOUSING10#11913L), sum(POP_2010#11916L)]\n",
      "Aggregate Attributes [3]: [ST_Union_Aggr(geometry#11894)#12041, sum(HOUSING10#11913L)#12043L, sum(POP_2010#11916L)#12045L]\n",
      "Results [5]: [ZCTA10#11924, COMM#11907, ST_Union_Aggr(geometry#11894)#12041 AS geometry#12042, sum(HOUSING10#11913L)#12043L AS Households#12044L, sum(POP_2010#11916L)#12045L AS Population#12046L]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#12082, Estimated Median Income#12084]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#12082, Estimated Median Income#12084]\n",
      "Condition : isnotnull(Zip Code#12082)\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [Zip Code#12082 AS ZIP#12093, cast(regexp_replace(Estimated Median Income#12084, [\\$,], , 1) as int) AS Median_Income#12094]\n",
      "Input [2]: [Zip Code#12082, Estimated Median Income#12084]\n",
      "\n",
      "(12) BroadcastExchange\n",
      "Input [2]: [ZIP#12093, Median_Income#12094]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=6987]\n",
      "\n",
      "(13) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#11924]\n",
      "Right keys [1]: [ZIP#12093]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [4]: [COMM#11907, geometry#12042, Population#12046L, (Households#12044L * cast(Median_Income#12094 as bigint)) AS Total_Income#12116L]\n",
      "Input [7]: [ZCTA10#11924, COMM#11907, geometry#12042, Households#12044L, Population#12046L, ZIP#12093, Median_Income#12094]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [4]: [COMM#11907, geometry#12042, Population#12046L, Total_Income#12116L]\n",
      "Keys [1]: [COMM#11907]\n",
      "Functions [3]: [partial_sum(Population#12046L), partial_sum(Total_Income#12116L), partial_st_union_aggr(geometry#12042, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@24712e69, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#12294L, sum#12296L, buf#12298]\n",
      "Results [4]: [COMM#11907, sum#12295L, sum#12297L, buf#12299]\n",
      "\n",
      "(16) Exchange\n",
      "Input [4]: [COMM#11907, sum#12295L, sum#12297L, buf#12299]\n",
      "Arguments: hashpartitioning(COMM#11907, 1000), ENSURE_REQUIREMENTS, [plan_id=6992]\n",
      "\n",
      "(17) ObjectHashAggregate\n",
      "Input [4]: [COMM#11907, sum#12295L, sum#12297L, buf#12299]\n",
      "Keys [1]: [COMM#11907]\n",
      "Functions [3]: [sum(Population#12046L), sum(Total_Income#12116L), st_union_aggr(geometry#12042, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@24712e69, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#12046L)#12133L, sum(Total_Income#12116L)#12135L, ST_Union_Aggr(geometry#12042)#12140]\n",
      "Results [4]: [COMM#11907, sum(Population#12046L)#12133L AS Population#12134L, sum(Total_Income#12116L)#12135L AS Total_Income#12136L, ST_Union_Aggr(geometry#12042)#12140 AS geometry#12141]\n",
      "\n",
      "(18) Filter\n",
      "Input [4]: [COMM#11907, Population#12134L, Total_Income#12136L, geometry#12141]\n",
      "Condition : isnotnull(geometry#12141)\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#55, LON#56]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LON), IsNotNull(LAT), Not(EqualTo(LON,0.0)), Not(EqualTo(LAT,0.0))]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#55, LON#56]\n",
      "Condition : ((((isnotnull(LON#56) AND isnotnull(LAT#55)) AND NOT (LON#56 = 0.0)) AND NOT (LAT#55 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#11851]\n",
      "Input [2]: [LAT#55, LON#56]\n",
      "\n",
      "(22) Scan csv \n",
      "Output [2]: [LAT#129, LON#130]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LON), IsNotNull(LAT), Not(EqualTo(LON,0.0)), Not(EqualTo(LAT,0.0))]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(23) Filter\n",
      "Input [2]: [LAT#129, LON#130]\n",
      "Condition : ((((isnotnull(LON#130) AND isnotnull(LAT#129)) AND NOT (LON#130 = 0.0)) AND NOT (LAT#129 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(24) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#12351]\n",
      "Input [2]: [LAT#129, LON#130]\n",
      "\n",
      "(25) Union\n",
      "\n",
      "(26) RangeJoin\n",
      "Arguments: geometry#12141: geometry, geom#11851: geometry, CONTAINS\n",
      "\n",
      "(27) Project\n",
      "Output [3]: [COMM#11907, Population#12134L, Total_Income#12136L]\n",
      "Input [5]: [COMM#11907, Population#12134L, Total_Income#12136L, geometry#12141, geom#11851]\n",
      "\n",
      "(28) HashAggregate\n",
      "Input [3]: [COMM#11907, Population#12134L, Total_Income#12136L]\n",
      "Keys [3]: [COMM#11907, Population#12134L, Total_Income#12136L]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#12292L]\n",
      "Results [4]: [COMM#11907, Population#12134L, Total_Income#12136L, count#12293L]\n",
      "\n",
      "(29) Exchange\n",
      "Input [4]: [COMM#11907, Population#12134L, Total_Income#12136L, count#12293L]\n",
      "Arguments: hashpartitioning(COMM#11907, Population#12134L, Total_Income#12136L, 1000), ENSURE_REQUIREMENTS, [plan_id=6999]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [4]: [COMM#11907, Population#12134L, Total_Income#12136L, count#12293L]\n",
      "Keys [3]: [COMM#11907, Population#12134L, Total_Income#12136L]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#12260L]\n",
      "Results [3]: [COMM#11907, format_number((cast(count(1)#12260L as double) / cast(Population#12134L as double)), 4) AS Crime Rate#12266, (cast(Total_Income#12136L as double) / cast(Population#12134L as double)) AS Estimated_Median_Income#12272]\n",
      "\n",
      "(31) Exchange\n",
      "Input [3]: [COMM#11907, Crime Rate#12266, Estimated_Median_Income#12272]\n",
      "Arguments: rangepartitioning(Estimated_Median_Income#12272 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=7002]\n",
      "\n",
      "(32) Sort\n",
      "Input [3]: [COMM#11907, Crime Rate#12266, Estimated_Median_Income#12272]\n",
      "Arguments: [Estimated_Median_Income#12272 DESC NULLS LAST], true, 0\n",
      "\n",
      "(33) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#11907, Crime Rate#12266, Estimated_Median_Income#12272]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "population_data.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da9968e2-fcc0-4c28-bb07-d5ac5463969e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "benchmark = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "662e07fe-2a37-46d5-b2ab-e41405cd95e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import time\n",
    "\n",
    "strategy_1 = \"SHUFFLE_HASH\"\n",
    "strategy_2 = \"SHUFFLE_HASH\"\n",
    "# We apply the hint to set the join strategy used\n",
    "# the code executes the same query as presented above\n",
    "spark.catalog.clearCache()\n",
    "start_time = time.time()\n",
    "\n",
    "joined_income = LA_areas.hint(strategy_1).join(income.hint(strategy_1), LA_areas[\"ZCTA10\"] == income[\"ZIP\"], \"inner\") \\\n",
    "    .drop(\"ZCTA10\", \"ZIP\") \\\n",
    "    .withColumn(\"Total_Income\", col(\"Households\") * col(\"Median_Income\")) \\\n",
    "    .drop(\"Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        f.sum(\"Population\").alias(\"Population\"),\n",
    "        f.sum(\"Total_Income\").alias(\"Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    ")\n",
    "\n",
    "population_data = joined_income.hint(strategy_2).join(crime_data.hint(strategy_2), ST_Within(crime_data.geom, joined_income.geometry), \"inner\") \\\n",
    "    .groupBy(\"COMM\",\"Population\",\"Total_Income\") \\\n",
    "    .agg(\n",
    "        f.count(col(\"*\")).alias(\"Total Crimes\")\n",
    "    ) \\\n",
    "    .withColumn(\"Crime Rate\",format_number((col(\"Total Crimes\")/col(\"Population\")),4)) \\\n",
    "    .withColumn(\"Estimated_Median_Income\", (col(\"Total_Income\") / col(\"Population\"))) \\\n",
    "    .drop(\"Total_Income\",\"Population\",\"Total crimes\") \\\n",
    "    .orderBy(col(\"Estimated_Median_Income\").desc())\n",
    "\n",
    "population_data.collect()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "benchmark.append((strategy_1, strategy_2, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8e46a39-9d7e-424e-be11-c2d36a3d6198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SHUFFLE_REPLICATE_NL', 'SHUFFLE_REPLICATE_NL', 46.515050411224365)\n",
      "('SHUFFLE_REPLICATE_NL', 'MERGE', 26.257415533065796)\n",
      "('SHUFFLE_REPLICATE_NL', 'BROADCAST', 26.699812412261963)\n",
      "('SHUFFLE_REPLICATE_NL', 'SHUFFLE_HASH', 177.74399709701538)\n",
      "('MERGE', 'SHUFFLE_REPLICATE_NL', 41.28805613517761)\n",
      "('MERGE', 'MERGE', 27.015713453292847)\n",
      "('MERGE', 'BROADCAST', 25.8994722366333)\n",
      "('MERGE', 'SHUFFLE_HASH', 29.319011926651)\n",
      "('BROADCAST', 'SHUFFLE_REPLICATE_NL', 31.50220274925232)\n",
      "('BROADCAST', 'MERGE', 24.24892282485962)\n",
      "('BROADCAST', 'BROADCAST', 29.62976360321045)\n",
      "('BROADCAST', 'SHUFFLE_HASH', 30.067917346954346)\n",
      "('SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL', 17.198349714279175)\n",
      "('SHUFFLE_HASH', 'MERGE', 23.190125703811646)\n",
      "('SHUFFLE_HASH', 'BROADCAST', 20.927560567855835)\n",
      "('SHUFFLE_HASH', 'SHUFFLE_HASH', 17.760035753250122)"
     ]
    }
   ],
   "source": [
    "for res in benchmark:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68656207-bd0e-40c9-8d34-47ec4558eb67",
   "metadata": {},
   "source": [
    "Query 4\n",
    "---\n",
    "\n",
    "Να βρεθεί το φυλετικό προφίλ των καταγεγραμμένων θυμάτων εγκλημάτων (Vict Descent) στο Los Angeles για το έτος 2015 στις 3 περιοχές με το υψηλότερο κατά κεφαλήν εισόδημα. Να γίνει το ίδιο για τις 3 περιοχές με το χαμηλότερο εισόδημα. Να χρησιμοποιήσετε την αντιστοίχιση των κωδικών\n",
    "καταγωγής με την πλήρη περιγραφή από το σύνολο δεδομενων Race and Ethnicity codes. Τα αποτελέσματα να τυπωθούν σε δύο ξεχωριστούς πίνακες από το υψηλότερο στο χαμηλότερο αριθμό θυμάτων ανά φυλετικό γκρουπ (δείτε παράδειγμα αποτελέσματος στον Πίνακα 3).\n",
    "\n",
    "---\n",
    "Question 4\n",
    "---\n",
    "\n",
    "Να υλοποιηθεί το Query 4 χρησιμοποιώντας το DataFrame ή SQL API. Να εκτελέσετε την υλοποίησή σας εφαρμόζοντας κλιμάκωση στο σύνολο των υπολογιστικών πόρων που θα χρησιμοποιήσετε: Συγκεκριμένα, καλείστε να εκτελέστε την υλοποίησή σας σε 2 executors με τα ακόλουθα configurations:\n",
    "- 1 core/2 GB memory\n",
    "- 2 cores/4GB memory\n",
    "- 4 cores/8GB memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "562eace3-5e0e-4425-bccd-8c062813c8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|649|\n",
      "|               Other| 72|\n",
      "|Hispanic/Latin/Me...| 66|\n",
      "|             Unknown| 38|\n",
      "|               Black| 37|\n",
      "|         Other Asian| 21|\n",
      "|American Indian/A...|  1|\n",
      "|             Chinese|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|2815|\n",
      "|               Black| 761|\n",
      "|               White| 330|\n",
      "|               Other| 187|\n",
      "|         Other Asian| 113|\n",
      "|             Unknown|  22|\n",
      "|American Indian/A...|  21|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|         AsianIndian|   1|\n",
      "|            Filipino|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken for Dataframe API using 1 core/2 GB memory: 46.36 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Question 4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "race_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "LA_areas = flattened_df.select(\"ZCTA10\",\"COMM\",\"POP_2010\",\"HOUSING10\",\"geometry\") \\\n",
    "                .filter(((col(\"POP_2010\")>0) & (col(\"HOUSING10\")>0)) & (trim(col(\"COMM\"))!=\"\")) \\\n",
    "                .groupBy([\"ZCTA10\",\"COMM\"]) \\\n",
    "                .agg(\n",
    "                    ST_Union_Aggr(\"geometry\").alias(\"geometry\"),\n",
    "                    f.sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                    f.sum(\"POP_2010\").alias(\"Population\")\n",
    "                )\n",
    "\n",
    "income = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(col(\"Estimated Median Income\"), r\"[\\$,]\", \"\").cast(\"int\")) \\\n",
    "    .select(\n",
    "        col(\"Zip Code\").alias(\"ZIP\"),\n",
    "        col(\"Estimated Median Income\").alias(\"Median_Income\")\n",
    "    )\n",
    "\n",
    "joined_income = LA_areas.join(income, LA_areas[\"ZCTA10\"] == income[\"ZIP\"], \"inner\") \\\n",
    "    .drop(\"ZCTA10\", \"ZIP\") \\\n",
    "    .withColumn(\"Total_Income\", col(\"Households\") * col(\"Median_Income\")) \\\n",
    "    .drop(\"Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        f.sum(\"Population\").alias(\"Population\"),\n",
    "        f.sum(\"Total_Income\").alias(\"Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    ) \\\n",
    "    .withColumn(\"Income_Per_Capita\", (col(\"Total_Income\") / col(\"Population\"))) \\\n",
    "    .select(\"COMM\", \"Income_Per_Capita\", \"geometry\")\n",
    "\n",
    "first_3 = joined_income.orderBy(col(\"Income_Per_Capita\").desc()).limit(3)\n",
    "\n",
    "last_3 = joined_income.orderBy(col(\"Income_Per_Capita\").asc()).limit(3)\n",
    "\n",
    "crime_data_2015 = crime_data.filter((substring(col(\"DATE OCC\"), 7, 4) == \"2015\") & (col(\"Vict Descent\") != \"\")) \\\n",
    "    .select(\"DATE OCC\", \"geom\", \"Vict Descent\")\n",
    "\n",
    "result_top_3 = first_3.join(crime_data_2015, ST_Within(crime_data_2015.geom, first_3.geometry), \"inner\") \\\n",
    "    .groupBy(\"Vict Descent\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"#\")\n",
    "    )\n",
    "\n",
    "final_res_top_3 = result_top_3.join(race_codes, result_top_3[\"`Vict Descent`\"] == race_codes[\"`Vict Descent`\"], \"inner\") \\\n",
    "    .select(race_codes[\"`Vict Descent Full`\"].alias(\"Victim Descent\"), \"#\") \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "final_res_top_3.show()\n",
    "\n",
    "\n",
    "result_last_3 = last_3.join(crime_data_2015, ST_Within(crime_data_2015.geom, last_3.geometry), \"inner\") \\\n",
    "    .groupBy(\"Vict Descent\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"#\")\n",
    "    )\n",
    "\n",
    "final_res_last_3 = result_last_3.join(race_codes, result_last_3[\"`Vict Descent`\"] == race_codes[\"`Vict Descent`\"], \"inner\") \\\n",
    "    .select(race_codes[\"`Vict Descent Full`\"].alias(\"Victim Descent\"), \"#\") \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "final_res_last_3.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using 1 core/2 GB memory: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d83c20d5-ee80-4eff-b9bb-03b5a5f43752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|649|\n",
      "|               Other| 72|\n",
      "|Hispanic/Latin/Me...| 66|\n",
      "|             Unknown| 38|\n",
      "|               Black| 37|\n",
      "|         Other Asian| 21|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|2815|\n",
      "|               Black| 761|\n",
      "|               White| 330|\n",
      "|               Other| 187|\n",
      "|         Other Asian| 113|\n",
      "|             Unknown|  22|\n",
      "|American Indian/A...|  21|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|         AsianIndian|   1|\n",
      "|            Filipino|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken for Dataframe API using 2 core/4 GB memory: 47.82 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Question 4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "race_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "LA_areas = flattened_df.select(\"ZCTA10\",\"COMM\",\"POP_2010\",\"HOUSING10\",\"geometry\") \\\n",
    "                .filter(((col(\"POP_2010\")>0) & (col(\"HOUSING10\")>0)) & (trim(col(\"COMM\"))!=\"\")) \\\n",
    "                .groupBy([\"ZCTA10\",\"COMM\"]) \\\n",
    "                .agg(\n",
    "                    ST_Union_Aggr(\"geometry\").alias(\"geometry\"),\n",
    "                    f.sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                    f.sum(\"POP_2010\").alias(\"Population\")\n",
    "                )\n",
    "\n",
    "income = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(col(\"Estimated Median Income\"), r\"[\\$,]\", \"\").cast(\"int\")) \\\n",
    "    .select(\n",
    "        col(\"Zip Code\").alias(\"ZIP\"),\n",
    "        col(\"Estimated Median Income\").alias(\"Median_Income\")\n",
    "    )\n",
    "\n",
    "joined_income = LA_areas.join(income, LA_areas[\"ZCTA10\"] == income[\"ZIP\"], \"inner\") \\\n",
    "    .drop(\"ZCTA10\", \"ZIP\") \\\n",
    "    .withColumn(\"Total_Income\", col(\"Households\") * col(\"Median_Income\")) \\\n",
    "    .drop(\"Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        f.sum(\"Population\").alias(\"Population\"),\n",
    "        f.sum(\"Total_Income\").alias(\"Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    ) \\\n",
    "    .withColumn(\"Income_Per_Capita\", (col(\"Total_Income\") / col(\"Population\"))) \\\n",
    "    .select(\"COMM\", \"Income_Per_Capita\", \"geometry\")\n",
    "\n",
    "first_3 = joined_income.orderBy(col(\"Income_Per_Capita\").desc()).limit(3)\n",
    "\n",
    "last_3 = joined_income.orderBy(col(\"Income_Per_Capita\").asc()).limit(3)\n",
    "\n",
    "crime_data_2015 = crime_data.filter((substring(col(\"DATE OCC\"), 7, 4) == \"2015\") & (col(\"Vict Descent\") != \"\")) \\\n",
    "    .select(\"DATE OCC\", \"geom\", \"Vict Descent\")\n",
    "\n",
    "result_top_3 = first_3.join(crime_data_2015, ST_Within(crime_data_2015.geom, first_3.geometry), \"inner\") \\\n",
    "    .groupBy(\"Vict Descent\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"#\")\n",
    "    )\n",
    "\n",
    "final_res_top_3 = result_top_3.join(race_codes, result_top_3[\"`Vict Descent`\"] == race_codes[\"`Vict Descent`\"], \"inner\") \\\n",
    "    .select(race_codes[\"`Vict Descent Full`\"].alias(\"Victim Descent\"), \"#\") \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "final_res_top_3.show()\n",
    "\n",
    "\n",
    "result_last_3 = last_3.join(crime_data_2015, ST_Within(crime_data_2015.geom, last_3.geometry), \"inner\") \\\n",
    "    .groupBy(\"Vict Descent\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"#\")\n",
    "    )\n",
    "\n",
    "final_res_last_3 = result_last_3.join(race_codes, result_last_3[\"`Vict Descent`\"] == race_codes[\"`Vict Descent`\"], \"inner\") \\\n",
    "    .select(race_codes[\"`Vict Descent Full`\"].alias(\"Victim Descent\"), \"#\") \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "final_res_last_3.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using 2 core/4 GB memory: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2663e787-c871-4c41-a3c5-061aacbce49f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|649|\n",
      "|               Other| 72|\n",
      "|Hispanic/Latin/Me...| 66|\n",
      "|             Unknown| 38|\n",
      "|               Black| 37|\n",
      "|         Other Asian| 21|\n",
      "|American Indian/A...|  1|\n",
      "|             Chinese|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|2815|\n",
      "|               Black| 761|\n",
      "|               White| 330|\n",
      "|               Other| 187|\n",
      "|         Other Asian| 113|\n",
      "|             Unknown|  22|\n",
      "|American Indian/A...|  21|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|         AsianIndian|   1|\n",
      "|            Filipino|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken for Dataframe API using 4 core/8 GB memory: 88.80 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Question 4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "race_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "LA_areas = flattened_df.select(\"ZCTA10\",\"COMM\",\"POP_2010\",\"HOUSING10\",\"geometry\") \\\n",
    "                .filter(((col(\"POP_2010\")>0) & (col(\"HOUSING10\")>0)) & (trim(col(\"COMM\"))!=\"\")) \\\n",
    "                .groupBy([\"ZCTA10\",\"COMM\"]) \\\n",
    "                .agg(\n",
    "                    ST_Union_Aggr(\"geometry\").alias(\"geometry\"),\n",
    "                    f.sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                    f.sum(\"POP_2010\").alias(\"Population\")\n",
    "                )\n",
    "\n",
    "income = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(col(\"Estimated Median Income\"), r\"[\\$,]\", \"\").cast(\"int\")) \\\n",
    "    .select(\n",
    "        col(\"Zip Code\").alias(\"ZIP\"),\n",
    "        col(\"Estimated Median Income\").alias(\"Median_Income\")\n",
    "    )\n",
    "\n",
    "joined_income = LA_areas.join(income, LA_areas[\"ZCTA10\"] == income[\"ZIP\"], \"inner\") \\\n",
    "    .drop(\"ZCTA10\", \"ZIP\") \\\n",
    "    .withColumn(\"Total_Income\", col(\"Households\") * col(\"Median_Income\")) \\\n",
    "    .drop(\"Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        f.sum(\"Population\").alias(\"Population\"),\n",
    "        f.sum(\"Total_Income\").alias(\"Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    ) \\\n",
    "    .withColumn(\"Income_Per_Capita\", (col(\"Total_Income\") / col(\"Population\"))) \\\n",
    "    .select(\"COMM\", \"Income_Per_Capita\", \"geometry\")\n",
    "\n",
    "first_3 = joined_income.orderBy(col(\"Income_Per_Capita\").desc()).limit(3)\n",
    "\n",
    "last_3 = joined_income.orderBy(col(\"Income_Per_Capita\").asc()).limit(3)\n",
    "\n",
    "crime_data_2015 = crime_data.filter((substring(col(\"DATE OCC\"), 7, 4) == \"2015\") & (col(\"Vict Descent\") != \"\")) \\\n",
    "    .select(\"DATE OCC\", \"geom\", \"Vict Descent\")\n",
    "\n",
    "result_top_3 = first_3.join(crime_data_2015, ST_Within(crime_data_2015.geom, first_3.geometry), \"inner\") \\\n",
    "    .groupBy(\"Vict Descent\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"#\")\n",
    "    )\n",
    "\n",
    "final_res_top_3 = result_top_3.join(race_codes, result_top_3[\"`Vict Descent`\"] == race_codes[\"`Vict Descent`\"], \"inner\") \\\n",
    "    .select(race_codes[\"`Vict Descent Full`\"].alias(\"Victim Descent\"), \"#\") \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "final_res_top_3.show()\n",
    "\n",
    "\n",
    "result_last_3 = last_3.join(crime_data_2015, ST_Within(crime_data_2015.geom, last_3.geometry), \"inner\") \\\n",
    "    .groupBy(\"Vict Descent\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"#\")\n",
    "    )\n",
    "\n",
    "final_res_last_3 = result_last_3.join(race_codes, result_last_3[\"`Vict Descent`\"] == race_codes[\"`Vict Descent`\"], \"inner\") \\\n",
    "    .select(race_codes[\"`Vict Descent Full`\"].alias(\"Victim Descent\"), \"#\") \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "final_res_last_3.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using 4 core/8 GB memory: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db80a66-4304-45a2-b4df-f2ab585f1829",
   "metadata": {},
   "source": [
    "Query 5\n",
    "---\n",
    "\n",
    "Nα υπολογιστεί, ανά αστυνομικό τμήμα, ο αριθμός εγκλημάτων που έλαβαν χώρα πλησιέστερα σε αυτό, καθώς και η μέση απόστασή του από τις τοποθεσίες όπου σημειώθηκαν τα συγκεκριμένα περιστατικά. Τα αποτελέσματα να εμφανιστούν ταξινομημένα κατά αριθμό περιστατικών, με φθίνουσα σειρά (δείτε παράδειγμα στον Πίνακα 4).\n",
    "\n",
    "---\n",
    "Question 5\n",
    "---\n",
    "\n",
    "Να υλοποιηθεί το Query 4 χρησιμοποιώντας το DataFrame ή SQL API. Να εκτελέσετε την υλοποίησή σας εφαρμόζοντας κλιμάκωση στο σύνολο των υπολογιστικών πόρων που θα χρησιμοποιήσετε: Συγκεκριμένα, καλείστε να εκτελέστε την υλοποίησή σας σε 2 executors με τα ακόλουθα configurations:\n",
    "- 2 exec/4 core/8 GB memory\n",
    "- 4 exec/2 cores/4GB memory\n",
    "- 8 exec/1 cores/2GB memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ad4990a-3b7c-40c8-8887-d5438435e25e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------+\n",
      "|        division|      avg_distance|     #|\n",
      "+----------------+------------------+------+\n",
      "|       HOLLYWOOD|2.0762639601787227|224340|\n",
      "|        VAN NUYS| 2.953369742819784|210134|\n",
      "|       SOUTHWEST| 2.191398805780885|188901|\n",
      "|        WILSHIRE|2.5926655329787778|185996|\n",
      "|     77TH STREET|1.7165449719700996|171827|\n",
      "|         OLYMPIC|1.7236036971780924|170897|\n",
      "| NORTH HOLLYWOOD|2.6430060941415667|167854|\n",
      "|         PACIFIC|3.8500706553079005|161359|\n",
      "|         CENTRAL|0.9924764374568925|153871|\n",
      "|         RAMPART|1.5345341879190053|152736|\n",
      "|       SOUTHEAST|2.4218662158881825|152176|\n",
      "|     WEST VALLEY| 3.035671216314081|138643|\n",
      "|         TOPANGA|3.2969548417555536|138217|\n",
      "|        FOOTHILL| 4.250921708424994|134896|\n",
      "|          HARBOR|3.7025615993565055|126747|\n",
      "|      HOLLENBECK| 2.680181237706821|115837|\n",
      "|WEST LOS ANGELES|2.7924572890341115|115781|\n",
      "|          NEWTON|1.6346357397097444|111110|\n",
      "|       NORTHEAST| 3.623665524604075|108109|\n",
      "|         MISSION| 3.690942614278606|103355|\n",
      "+----------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for Dataframe API using 2exec/4 core/8 GB memory: 23.82 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Question 4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "police_stations = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed(\"X\", \"LON\") \\\n",
    "    .withColumnRenamed(\"Y\", \"LAT\") \\\n",
    "    .withColumn(\"station_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Cross-join police stations and crime data, calculate distances\n",
    "distance_df = crime_data.crossJoin(police_stations) \\\n",
    "    .withColumn(\"distance\", f.expr(\"ST_DistanceSphere(station_geom, geom) / 1000\"))\n",
    "\n",
    "# Find the nearest station for each crime\n",
    "closest_station_df = distance_df.groupBy(\"DR_NO\", \"geom\") \\\n",
    "    .agg(f.expr(\"MIN(distance)\").alias(\"min_distance\"))\n",
    "\n",
    "\n",
    "\n",
    "# Join back with the full distance DataFrame to get station details\n",
    "joined_df = distance_df.join(closest_station_df, \n",
    "                             (distance_df[\"DR_NO\"] == closest_station_df[\"DR_NO\"]) & \n",
    "                             (distance_df[\"distance\"] == closest_station_df[\"min_distance\"]),\n",
    "                             \"inner\")\n",
    "\n",
    "# Aggregate results for each police station\n",
    "result_df = joined_df.groupBy(\"DIVISION\") \\\n",
    "    .agg(\n",
    "        f.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        f.count(\"*\").alias(\"#\"),\n",
    "    ) \\\n",
    "    .orderBy(col(\"#\").desc()) \\\n",
    "    .withColumnRenamed(\"DIVISION\", \"division\")\n",
    "\n",
    "result_df.show()\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using 2exec/4 core/8 GB memory: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b545dab-2ca8-40a2-a8dc-72601dc1b1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------+\n",
      "|        division|      avg_distance|     #|\n",
      "+----------------+------------------+------+\n",
      "|       HOLLYWOOD|2.0762639601787214|224340|\n",
      "|        VAN NUYS|2.9533697428197825|210134|\n",
      "|       SOUTHWEST|2.1913988057808855|188901|\n",
      "|        WILSHIRE| 2.592665532978778|185996|\n",
      "|     77TH STREET|1.7165449719701007|171827|\n",
      "|         OLYMPIC|1.7236036971780937|170897|\n",
      "| NORTH HOLLYWOOD| 2.643006094141568|167854|\n",
      "|         PACIFIC|3.8500706553079014|161359|\n",
      "|         CENTRAL|0.9924764374568922|153871|\n",
      "|         RAMPART|1.5345341879190055|152736|\n",
      "|       SOUTHEAST| 2.421866215888182|152176|\n",
      "|     WEST VALLEY|3.0356712163140815|138643|\n",
      "|         TOPANGA|3.2969548417555545|138217|\n",
      "|        FOOTHILL|4.2509217084249915|134896|\n",
      "|          HARBOR|3.7025615993565055|126747|\n",
      "|      HOLLENBECK| 2.680181237706821|115837|\n",
      "|WEST LOS ANGELES|2.7924572890341137|115781|\n",
      "|          NEWTON| 1.634635739709745|111110|\n",
      "|       NORTHEAST|3.6236655246040743|108109|\n",
      "|         MISSION|3.6909426142786055|103355|\n",
      "+----------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for Dataframe API using 4exec/2 core/4 GB memory: 31.21 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Question 4\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "police_stations = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed(\"X\", \"LON\") \\\n",
    "    .withColumnRenamed(\"Y\", \"LAT\") \\\n",
    "    .withColumn(\"station_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Cross-join police stations and crime data, calculate distances\n",
    "distance_df = crime_data.crossJoin(police_stations) \\\n",
    "    .withColumn(\"distance\", f.expr(\"ST_DistanceSphere(station_geom, geom) / 1000\"))\n",
    "\n",
    "# Find the nearest station for each crime\n",
    "closest_station_df = distance_df.groupBy(\"DR_NO\", \"geom\") \\\n",
    "    .agg(f.expr(\"MIN(distance)\").alias(\"min_distance\"))\n",
    "\n",
    "\n",
    "\n",
    "# Join back with the full distance DataFrame to get station details\n",
    "joined_df = distance_df.join(closest_station_df, \n",
    "                             (distance_df[\"DR_NO\"] == closest_station_df[\"DR_NO\"]) & \n",
    "                             (distance_df[\"distance\"] == closest_station_df[\"min_distance\"]),\n",
    "                             \"inner\")\n",
    "\n",
    "# Aggregate results for each police station\n",
    "result_df = joined_df.groupBy(\"DIVISION\") \\\n",
    "    .agg(\n",
    "        f.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        f.count(\"*\").alias(\"#\"),\n",
    "    ) \\\n",
    "    .orderBy(col(\"#\").desc()) \\\n",
    "    .withColumnRenamed(\"DIVISION\", \"division\")\n",
    "\n",
    "result_df.show()\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using 4exec/2 core/4 GB memory: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99653f95-958f-4bfb-965b-c2529dc98213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------+\n",
      "|        division|      avg_distance|     #|\n",
      "+----------------+------------------+------+\n",
      "|       HOLLYWOOD|2.0762639601787236|224340|\n",
      "|        VAN NUYS| 2.953369742819782|210134|\n",
      "|       SOUTHWEST|2.1913988057808846|188901|\n",
      "|        WILSHIRE|2.5926655329787778|185996|\n",
      "|     77TH STREET|1.7165449719701003|171827|\n",
      "|         OLYMPIC|1.7236036971780935|170897|\n",
      "| NORTH HOLLYWOOD|2.6430060941415676|167854|\n",
      "|         PACIFIC|3.8500706553078983|161359|\n",
      "|         CENTRAL|0.9924764374568925|153871|\n",
      "|         RAMPART|1.5345341879190062|152736|\n",
      "|       SOUTHEAST|2.4218662158881834|152176|\n",
      "|     WEST VALLEY| 3.035671216314082|138643|\n",
      "|         TOPANGA|3.2969548417555554|138217|\n",
      "|        FOOTHILL| 4.250921708424992|134896|\n",
      "|          HARBOR|3.7025615993565077|126747|\n",
      "|      HOLLENBECK|2.6801812377068197|115837|\n",
      "|WEST LOS ANGELES| 2.792457289034112|115781|\n",
      "|          NEWTON|1.6346357397097442|111110|\n",
      "|       NORTHEAST|3.6236655246040765|108109|\n",
      "|         MISSION|3.6909426142786064|103355|\n",
      "+----------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for Dataframe API using 8exec/1 core/2 GB memory: 18.53 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Question 4\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "police_stations = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed(\"X\", \"LON\") \\\n",
    "    .withColumnRenamed(\"Y\", \"LAT\") \\\n",
    "    .withColumn(\"station_geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Cross-join police stations and crime data, calculate distances\n",
    "distance_df = crime_data.crossJoin(police_stations) \\\n",
    "    .withColumn(\"distance\", f.expr(\"ST_DistanceSphere(station_geom, geom) / 1000\"))\n",
    "\n",
    "# Find the nearest station for each crime\n",
    "closest_station_df = distance_df.groupBy(\"DR_NO\", \"geom\") \\\n",
    "    .agg(f.expr(\"MIN(distance)\").alias(\"min_distance\"))\n",
    "\n",
    "\n",
    "\n",
    "# Join back with the full distance DataFrame to get station details\n",
    "joined_df = distance_df.join(closest_station_df, \n",
    "                             (distance_df[\"DR_NO\"] == closest_station_df[\"DR_NO\"]) & \n",
    "                             (distance_df[\"distance\"] == closest_station_df[\"min_distance\"]),\n",
    "                             \"inner\")\n",
    "\n",
    "# Aggregate results for each police station\n",
    "result_df = joined_df.groupBy(\"DIVISION\") \\\n",
    "    .agg(\n",
    "        f.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        f.count(\"*\").alias(\"#\"),\n",
    "    ) \\\n",
    "    .orderBy(col(\"#\").desc()) \\\n",
    "    .withColumnRenamed(\"DIVISION\", \"division\")\n",
    "\n",
    "result_df.show()\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for Dataframe API using 8exec/1 core/2 GB memory: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
